.. _topic-guides_hp-tuning-algorithms:

Hyperparameter Tuning Algorithms
================================

In the model development lifecycle, *hyperparameter tuning* is the process of
homing in on the features, model architecture, and training process parameters
that yield an effective model. Hyperparameter tuning is a :ref:`challenging
problem<topic-guides_hp-tuning-basics-difficulty>` in deep learning; however,
machine learning engineers employ a variety of approaches to optimize
hyperparameters.  The most common hyperparameter tuning techniques include:

#. **Manual** tuning is a time-consuming approach where the machine learning
   engineer tweaks hyperparameters by hand based on incremental results and
   intuition. Computational resource utilization is often inefficient.
#. **Grid** search evaluates all possible hyperparameter configurations and
   returns the best. The algorithm typically searches numeric floating point
   hyperparameters at fixed or logarithmic intervals. Grid search may not be
   feasible since the number of possible configurations grows exponentially in
   the number of hyperparameters. Long model training cycles further hinder the
   prospect of covering the entire search space within a reasonable timeframe.
   Grid search wastes resources optimizing nuisance hyperparameters that have
   little to no impact on model performance.
#. **Random** search evaluates a set of hyperparameter configurations chosen at
   random and returns the best. Machine learning engineers typically prefer
   random search over grid search since it isn't prone to optimizing nuisance
   hyperparameters.
#. **Population-based training (PBT)** begins as random search but periodically
   replaces low-performing hyperparameter configurations with ones *near* the
   high-performing points in the hyperparameter space. PBT assumes that model
   performance is continuous within small pockets of the hyperparameter space.
#. **Adaptive** downsampling algorithms begin as random search but periodically
   abandon low-performing configurations. Partial training lets us explore
   significantly more of the hyperparameter search space than algorithms that
   fully train the hyperparameter configurations being considered. Thus,
   adaptive downsampling typically finds a high-performing model in
   significantly less time than alternatives like random or grid search.
   Moreover, in contrast to PBT, it doesn't assume local continuity of model
   performance. Two prototypical approaches to adaptive downsampling are
   `Successive Halving <https://arxiv.org/pdf/1502.07943.pdf>`_ (SHA) and
   `Hyperband <https://arxiv.org/pdf/1502.07943.pdf>`_, an extension of SHA.


The following table summarizes how each method fares across different
optimization algorithm attributes.

+-------------------------------------+--------+--------+--------+--------+----------+
|                                     | Manual | Grid   | Random | PBT    | Adaptive |
+-------------------------------------+--------+--------+--------+--------+----------+
| Implementation Simplicity           | ✅     | ✅     | ✅     |        |          |
+-------------------------------------+--------+--------+--------+--------+----------+
| Efficient Resource Utilization      |        |        |        |        | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+
| Ignores Nuisance Hyperparameters    |        |        | ✅     | ✅     | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+
| Exploits Past Trials                |        |        |        | ✅     | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+
| Minimal Assumptions                 |        | ✅     | ✅     |        | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+
| Effective When Training Is Quick    |        | ✅     | ✅     | ✅     | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+
| Effective With Many Hyperparameters |        |        |        |        | ✅       |
+-------------------------------------+--------+--------+--------+--------+----------+

Next Steps in Determined
------------------------
- :ref:`topic-guides_hp-tuning-det`
- :ref:`topic-guides_yaml`
